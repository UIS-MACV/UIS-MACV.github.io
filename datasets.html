---
layout: default
title: "Datasets"
img:
  img01:  https://sites.google.com/a/saber.uis.edu.co/macv/_/rsrc/1541393109591/home/Fig1.png
  img02:  https://sites.google.com/a/saber.uis.edu.co/macv/_/rsrc/1541393113871/home/Fig2.png
  img03:  https://sites.google.com/a/saber.uis.edu.co/macv/_/rsrc/1541393118627/home/Fig3.png
  img04:  https://sites.google.com/a/saber.uis.edu.co/macv/_/rsrc/1541393122371/home/Fig4.png
  img05:  https://sites.google.com/a/saber.uis.edu.co/macv/_/rsrc/1541393124933/home/Fig5.png
---

<!-- Page Content -->
<div class="container">
  <ul>
    <li>
      <h3>Markerless Gait Dataset</h3>
      <a href="https://drive.google.com/file/d/1ijiqzIF0nbcaGhOfe0fJjc77cVnF7lWI/view?usp=sharing" target="_blank">Download dataset</a>
      <br><br><br><br>
    </li>
    <li>
      <h3>RGB-D Dataset</h3>
      <a href="https://drive.google.com/file/d/1KVhftC1SeKXweaDV8xjO0vm74-19URf5/view?usp=sharing" target="_blank">Download dataset</a>
      <br><br>
      <img src="https://sites.google.com/a/saber.uis.edu.co/macv/_/rsrc/1541393109591/home/Fig1.png" width="670" />
      <div style="font-style:italic;">Figure 1. Scene flow. The 3D motion between a pair of consecutive RGB-D images is obtained from the scene flow information. Please visit the site: to observe the image in color XX.<br>Source: The authors.</div>
      <br><br>
      <img src="https://sites.google.com/a/saber.uis.edu.co/macv/_/rsrc/1541393113871/home/Fig2.png" width="670" />
      <div style="font-style:italic;">Figure 2. motion trajectories. In left subplot is illustrated the RGB-D information of captured hand. Then, between consecutive frames is computed a scene flow (middle subplot) that recovers the velocity of hand and hence its main geometrical structure. From such scene flow is tracked 3D points along video to obtained long trajectories as illustrated in right subplot. In this plot color of trajectories represent the depth displacement, being green a color that represent that trajectories are close to the camera, while red the trajectories are far. Please visit the site: to observe the image in color XX.<br>Source: The authors.</div>
      <br><br>
      <img src="https://sites.google.com/a/saber.uis.edu.co/macv/_/rsrc/1541393118627/home/Fig3.png" width="670" />
      <div style="font-style:italic;">Figure 3. Recognition from proposed trajectories. (a) RGB-D acquisitions. (b) The dense scene flow is obtained and the trajectories are calculated. (c) From long trajectories, a set of kinematics primitives are computed to coded motion information that can be associated with gestures. In this work was computed mean and standard deviation velocities, as well as the curvature and torsion of each trajectory (d) This local kinematic primitives as coded as motion words into a scheme of bag-of-kinematics-words and then occurrence histograms are computed. Such histograms are mapped to SVM to predict the gesture. Please visit the site: to observe the image in color XX.<br>Source: The authors.</div>
      <br><br>
      <img src="https://sites.google.com/a/saber.uis.edu.co/macv/_/rsrc/1541393122371/home/Fig4.png" width="670" />
      <div style="font-style:italic;">Figure 4. Dataset to evaluate the proposed approach. This dataset is available at: Biv2Lab repository (https://uis-macv.github.io/datasets.html). In the first row it is shown the first frame of each one of the actions in the intensity channel, and their corresponding depth maps in the second row. A total of five different gestures were computed into a semi-controlled scenario. The 3D motions as representative keys of the recorded gestures. Please visit the site: to observe the image in color XX.<br>Source: The authors.</div>
      <br><br>
      <img src="https://sites.google.com/a/saber.uis.edu.co/macv/_/rsrc/1541393124933/home/Fig5.png" width="670" />
      <div style="font-style:italic;">Figure 5. 3D trajectories obtained for the gesture “ring”. Color represents the displacement in depth coded in the color-map shown in the color-bar. Please visit the site: to observe the image in color and appreciate the difference in depth with respect to the camera, achieved by the proposed approach.<br>Source: The authors.</div>
    </li>
  </ul>
</div>
